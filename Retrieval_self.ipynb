{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# Two-Tower Model for Recommendation Retrieval\n",
    "\n",
    "This notebook implements a two-tower embedding model for recommendation retrieval using FAISS for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "teYMjFEBiCSZ",
    "outputId": "c1fbc106-77df-42ba-c32b-924a59f999b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "#users: 1210271, #items: 212506\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load interaction data (updated path)\n",
    "df = pd.read_csv(\"dataset/amazon-beauty/amazon-beauty-train.inter\", sep=\"\\t\", dtype=str)\n",
    "\n",
    "# Keep positive interactions\n",
    "df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df = df[df[\"label\"] == 1]\n",
    "\n",
    "# Map user/item IDs to indices\n",
    "user2idx = {u: idx for idx, u in enumerate(df[\"user_id\"].unique())}\n",
    "item2idx = {i: idx for idx, i in enumerate(df[\"item_id\"].unique())}\n",
    "idx2item = {idx: item_id for item_id, idx in item2idx.items()}  # Reverse mapping for recommendations\n",
    "\n",
    "df[\"user_idx\"] = df[\"user_id\"].map(user2idx)\n",
    "df[\"item_idx\"] = df[\"item_id\"].map(item2idx)\n",
    "\n",
    "num_users = len(user2idx)\n",
    "num_items = len(item2idx)\n",
    "print(f\"#users: {num_users}, #items: {num_items}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User tower: self.user_emb maps each user_idx to a vector u\n",
    "\n",
    "Item tower: self.item_emb maps each item_idx to a vector i\n",
    "\n",
    "Score: Dot product <u,i> is the relevance score for (user, item)\n",
    "\n",
    "Purpose of the Two‑Tower Embedding Model:\n",
    "\n",
    "Retrieve relevant items for a user from a very large catalog\n",
    "\n",
    "At training time:\n",
    "\n",
    "Show the model positive interactions (user, item) with label 1.\n",
    "\n",
    "It learns user and item embeddings such that user vectors are close to their positive items.\n",
    "\n",
    "At inference time:\n",
    "\n",
    "For a given user, compute their embedding once.\n",
    "\n",
    "Then retrieve the closest item embeddings (by dot product / cosine similarity).\n",
    "\n",
    "Why use this architecture for retrieval:\n",
    "\n",
    "Decoupled towers: User and item embeddings are learned separately (only combined with a simple dot product).\n",
    "\n",
    "Precomputation: precompute and store all item embeddings once\n",
    "\n",
    "Fast scoring: A user’s vector vs. all items is just many dot products – perfect for FAISS / ANN.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_UjynAN7k_o3"
   },
   "outputs": [],
   "source": [
    "class InterDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df[\"user_idx\"].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df[\"item_idx\"].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx]\n",
    "\n",
    "dataset = InterDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why batch size choose 256: \n",
    "Embedding dimension: 64\n",
    "\n",
    "With batch_size=256:\n",
    "\n",
    "User embeddings: 256 × 64 = 16,384 floats\n",
    "\n",
    "Item embeddings: 256 × 64 = 16,384 floats\n",
    "\n",
    "Total per batch: ~32K floats ≈ 128 KB \n",
    "\n",
    "shuffle=True → randomly reorders samples each epoch\n",
    "\n",
    "if shuffle=False, samples stay in original order\n",
    "\n",
    "Model sees all of user1's items together → Model might memorize user1's pattern before seeing others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BJXUS72iiH2F"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "embedding_dim = 64\n",
    "\n",
    "class TwoTowerModel(nn.Module): # TwoTowerModel inherits from  nn.Module(PyTorch's base class for neural networks)\n",
    "    def __init__(self, num_users, num_items, embedding_dim):\n",
    "        super().__init__() # super() refers to the parent class (nn.Module), super().__init__() calls the parent's __init__() method\n",
    "        self.user_emb = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        u = self.user_emb(user_idx)\n",
    "        i = self.item_emb(item_idx)\n",
    "        # Dot product for retrieval score\n",
    "        return (u * i).sum(dim=1)\n",
    "\n",
    "    def get_user_embedding(self, user_idx):\n",
    "        return self.user_emb(user_idx)\n",
    "\n",
    "    def get_item_embedding(self, item_idx):\n",
    "        return self.item_emb(item_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: learn two sets of vectors:\n",
    "\n",
    "One vector for each user\n",
    "\n",
    "One vector for each item\n",
    "\n",
    "For a pair (user, item), it:\n",
    "\n",
    "Looks up the user embedding u\n",
    "\n",
    "Looks up the item embedding i\n",
    "\n",
    "Computes a score = dot product u⋅i\n",
    "\n",
    "→ higher score = model thinks this user will like this item more.\n",
    "Later, use the user vector to find top‑K closest item vectors with FAISS → these are the recommended items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IkIA4t-aiIcp"
   },
   "outputs": [],
   "source": [
    "# Use TwoTowerModel defined in previous cell\n",
    "# Initialize model and move to device\n",
    "model = TwoTowerModel(num_users, num_items, embedding_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer： updates model parameters (weights) to reduce the loss during training，The optimizer updates user_emb.weight, item_emb.weight during training\n",
    "\n",
    "a loss function (criterion):\n",
    "A loss function measures how far predictions are from targets. The optimizer minimizes this loss\n",
    "\n",
    "BCEWithLogitsLoss: BCEWithLogitsLoss = BCE(Sigmoid(logits), targets)\n",
    "Binary Cross-Entropy Loss with Logits combines:\n",
    "Sigmoid activation (logits(raw score)→ probabilities)\n",
    "Binary cross-entropy loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4aQswoei-1q",
    "outputId": "c63fbe6a-a82f-4748-cc99-7decc788be8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1 done - Average Loss: 3.2218\n",
      "Epoch 2 done - Average Loss: 2.7125\n",
      "Epoch 3 done - Average Loss: 2.3158\n",
      "Epoch 4 done - Average Loss: 2.0303\n",
      "Epoch 5 done - Average Loss: 1.8398\n",
      "Epoch 6 done - Average Loss: 1.7123\n",
      "Epoch 7 done - Average Loss: 1.6306\n",
      "Epoch 8 done - Average Loss: 1.5773\n",
      "Epoch 9 done - Average Loss: 1.5416\n",
      "Epoch 10 done - Average Loss: 1.5159\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Starting training\")\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    for batch_users, batch_items in dataloader:\n",
    "        # Move to device\n",
    "        batch_users = batch_users.to(device) \n",
    "        batch_items = batch_items.to(device)\n",
    "        \n",
    "        # Positive scores\n",
    "        pos_scores = model(batch_users, batch_items)\n",
    "\n",
    "        # Negative sampling: random items for each user\n",
    "        neg_items = torch.randint(0, num_items, batch_items.shape, device=device) #For each user in the batch, randomly picks an item ID in [0, num_items). These random (user, neg_item) pairs are treated as non-relevant.\n",
    "        neg_scores = model(batch_users, neg_items)\n",
    "\n",
    "        \n",
    "        # 2. Forward pass (get logits), Raw dot product scores, scores shape: [batch_size] - e.g., [256] raw numbers\n",
    "        scores = torch.cat([pos_scores, neg_scores], dim=0)\n",
    "        # 3. Build labels: 1 for pos, 0 for neg\n",
    "        labels = torch.cat([\n",
    "            torch.ones_like(pos_scores),\n",
    "            torch.zeros_like(neg_scores),\n",
    "        ], dim=0).float()\n",
    "\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "         \n",
    "        \n",
    "        loss = criterion(scores, labels) # 4. Compute loss\n",
    "        loss.backward() # 5. Backpropagate compute gradients\n",
    "        optimizer.step() # 6. Update weights (Adam)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    print(f\"Epoch {epoch+1} done - Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of negative sampling:\n",
    "\n",
    "data only has positive interactions (user clicked/rated/bought item).\n",
    "Without negatives, the model only sees “this pair is good” and never “this pair is bad”.\n",
    "\n",
    "Negative sampling creates fake negative examples by pairing each user with random items they did not interact with.\n",
    "\n",
    "The loss then pushes:\n",
    "\n",
    "positive pairs: scores ↑\n",
    "\n",
    "negative pairs: scores ↓\n",
    "\n",
    "This makes the embedding space discriminative: user vectors are close to their interacted items and far from random items.\n",
    "It approximates a ranking objective (relevant > non-relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build FAISS Index for Fast Retrieval\n",
    "\n",
    "The next cell will:\n",
    "1. Extract user and item embeddings from the trained model\n",
    "2. Build a FAISS index over Item Embeddings for efficient similarity search ( fast nearest‑neighbor search on large sets of vectors)\n",
    "3. Enable fast retrieval of top-K recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5lfyOJS6jJa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User embeddings shape: (1210271, 64)\n",
      "Item embeddings shape: (212506, 64)\n",
      "Saved user_embeddings.npy and item_embeddings.npy\n",
      "Saved normalized embeddings\n",
      "Training FAISS index\n",
      "Adding items to index\n",
      "FAISS index ready: 212,506 items\n",
      "Saved faiss_item_index.bin\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Extract and Save Embeddings\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    user_embeddings_raw = model.user_emb.weight.detach().cpu().numpy().astype('float32')\n",
    "    item_embeddings_raw = model.item_emb.weight.detach().cpu().numpy().astype('float32')\n",
    "\n",
    "print(f\"User embeddings shape: {user_embeddings_raw.shape}\")\n",
    "print(f\"Item embeddings shape: {item_embeddings_raw.shape}\")\n",
    "\n",
    "''' Example of user embedding\n",
    "User 0 → [0.12, -0.45, 0.78, 0.33, -0.21, ..., 0.56]  ← 64 numbers\n",
    "Training Data:\n",
    "  User 0 bought: lipstick, foundation, mascara\n",
    "  User 1 bought: shampoo, conditioner, hair gel\n",
    "\n",
    "Model learns:\n",
    "  User 0 embedding → close to makeup item embeddings\n",
    "  User 1 embedding → close to haircare item embeddings\n",
    "'''\n",
    "\n",
    "# Save raw embeddings\n",
    "np.save(\"user_embeddings.npy\", user_embeddings_raw)\n",
    "np.save(\"item_embeddings.npy\", item_embeddings_raw)\n",
    "print(\"Saved user_embeddings.npy and item_embeddings.npy\")\n",
    "\n",
    "# Create Normalized Copies for FAISS\n",
    "user_embeddings = user_embeddings_raw.copy()\n",
    "item_embeddings = item_embeddings_raw.copy()\n",
    "\n",
    "# Normalize for cosine similarity (modifies in place)\n",
    "faiss.normalize_L2(item_embeddings)\n",
    "faiss.normalize_L2(user_embeddings)\n",
    "\n",
    "# save normalized embeddings for later use\n",
    "np.save(\"user_embeddings_normalized.npy\", user_embeddings)\n",
    "np.save(\"item_embeddings_normalized.npy\", item_embeddings)\n",
    "print(\"Saved normalized embeddings\")\n",
    "\n",
    "# Build FAISS Index (CPU - IVFFlat)\n",
    "embedding_dim = item_embeddings.shape[1] # shape[0]: Number of items(212,506), shape[1]: Embedding dimension (64)\n",
    "\n",
    "# Why Normalize for Cosine Similarity?\n",
    "# Normalization + Inner Product = Cosine Similarity\n",
    "# This makes FAISS faster while still computing cosine similarity\n",
    "# If vectors are normalized (length = 1), then:\n",
    "# ||A|| = 1  and  ||B|| = 1\n",
    "\n",
    "# cosine_similarity(A, B) = (A · B) / (1 × 1) = A · B\n",
    "# Cosine similarity becomes just the dot product\n",
    "\n",
    "nlist = 1500  # Number of clusters\n",
    "\n",
    "# finds nearest cluster center\n",
    "quantizer = faiss.IndexFlatIP(embedding_dim)\n",
    "# Create IVF index using the quantizer\n",
    "index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "# Train the index (learns 1500 cluster centers)\n",
    "print(\"Training FAISS index\")\n",
    "index.train(item_embeddings)\n",
    "\n",
    "# Add items to index (assigns each item to a cluster)\n",
    "print(\"Adding items to index\")\n",
    "index.add(item_embeddings)\n",
    "\n",
    "# Set search parameter: how many clusters to search\n",
    "index.nprobe = 100  # Higher = more accurate, slower\n",
    "\n",
    "print(f\"FAISS index ready: {index.ntotal:,} items\")\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, \"faiss_item_index.bin\")\n",
    "print(\"Saved faiss_item_index.bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Hit Rate @K for Two-Tower Retrieval\n",
    "\n",
    "This cell evaluates how well the trained two-tower + FAISS retrieval model recovers the true clicked items on the test set using HR@K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test interactions from: dataset/amazon-beauty/amazon-beauty-test.inter\n",
      "Total positive test interactions: 328,812\n",
      "Unique test users: 322,870\n",
      "Unique test items: 108,510\n",
      "Subsampled users: 5,000 (from 322,870)\n",
      "\n",
      "Two-tower retrieval performance (Hit Rate):\n",
      "HR@10: 0.0002\n",
      "HR@20: 0.0002\n",
      "HR@50: 0.0004\n"
     ]
    }
   ],
   "source": [
    "# Evaluate two-tower retrieval using HR@K on test set\n",
    "import numpy as np\n",
    "\n",
    "# Load test interactions\n",
    "test_path = \"dataset/amazon-beauty/amazon-beauty-test.inter\"\n",
    "print(f\"Loading test interactions from: {test_path}\")\n",
    "\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\", dtype=str)\n",
    "\n",
    "# Keep positive interactions\n",
    "test_df[\"label\"] = pd.to_numeric(test_df[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "test_df = test_df[test_df[\"label\"] == 1].copy()\n",
    "\n",
    "print(f\"Total positive test interactions: {len(test_df):,}\")\n",
    "print(f\"Unique test users: {test_df['user_id'].nunique():,}\")\n",
    "print(f\"Unique test items: {test_df['item_id'].nunique():,}\")\n",
    "\n",
    "\n",
    "\"\"\"Compute HR@K for two-tower + FAISS retrieval.\n",
    "\n",
    "    Args:\n",
    "        test_df: DataFrame with columns [user_id, item_id, label].\n",
    "        ks: tuple of K values to evaluate.\n",
    "        max_users: optional cap on number of users to speed up evaluation.\n",
    "\"\"\"\n",
    "def evaluate_hr_at_k(test_df, ks=(10, 20, 50), max_users=5000):\n",
    "    \n",
    "    hr = {k: 0 for k in ks}\n",
    "    total = 0\n",
    "    max_k = max(ks)\n",
    "    \n",
    "    test_users = test_df['user_id'].unique()\n",
    "    if len(test_users) > max_users:\n",
    "        test_users = np.random.choice(test_users, max_users, replace=False)\n",
    "        print(f\"Subsampled users: {len(test_users):,} (from {test_df['user_id'].nunique():,})\")\n",
    "    \n",
    "    for user_id in test_users:\n",
    "        if user_id not in user2idx: # Users Must Be in Training Data, limitation of two-tower model: Cold Start Problem: cannot recommend for new users\n",
    "            continue\n",
    "        \n",
    "        user_idx = user2idx[user_id]\n",
    "        \n",
    "        true_items = test_df[test_df['user_id'] == user_id]['item_id'].values\n",
    "        true_item_indices = [item2idx[item] for item in true_items if item in item2idx]\n",
    "        \n",
    "        if not true_item_indices:\n",
    "            continue\n",
    "        \n",
    "        # user_embeddings is already normalized\n",
    "        query_emb = user_embeddings[user_idx:user_idx+1]\n",
    "        \n",
    "        distances, indices = index.search(query_emb, k=max_k)\n",
    "        retrieved = indices[0]\n",
    "        \n",
    "        total += 1\n",
    "        \n",
    "        for k in ks:\n",
    "            if any(item_idx in retrieved[:k] for item_idx in true_item_indices):\n",
    "                hr[k] += 1\n",
    "    \n",
    "    for k in ks:\n",
    "        hr[k] = hr[k] / total if total > 0 else 0\n",
    "    \n",
    "    return hr\n",
    "\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "hr = evaluate_hr_at_k(test_df, ks=(10, 20, 50), max_users=5000)\n",
    "print(\"\\nTwo-tower retrieval performance (Hit Rate):\")\n",
    "for k in sorted(hr.keys()):\n",
    "    print(f\"HR@{k}: {hr[k]:.4f}\")\n",
    "\n",
    "'''INTERPRETATION\n",
    "    two-tower model is performing barely better than random chance and worse than the simple baselines.\n",
    "    The model is not learning that users should be similar to their interacted items\n",
    "    The reason might be BCEWithLogitsLoss with random negatives isn't strong enough\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Embedding Norms ===\n",
      "User embedding norms (first 5): [0.99999994 1.         0.99999994 1.         1.        ]\n",
      "Item embedding norms (first 5): [0.99999994 1.         0.99999994 1.         0.99999994]\n",
      "Should be ~1.0 if normalized\n",
      "\n",
      "=== Sample Search ===\n",
      "Top 10 similarities: [0.5199655  0.5043913  0.50290835 0.49944636 0.4846565  0.48275822\n",
      " 0.47616628 0.47403243 0.46810657 0.46567205]\n",
      "Top 10 item indices: [172838   6284  61912  92566   5786 200979 162254 148239  17949 148637]\n",
      "Similarities should be between 0 and 1, with variation\n",
      "\n",
      "=== Ground Truth Check ===\n",
      "User 2238 test items: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check 1: Are embeddings normalized?\n",
    "print(\"=== Embedding Norms ===\")\n",
    "user_norms = np.linalg.norm(user_embeddings[:5], axis=1)\n",
    "item_norms = np.linalg.norm(item_embeddings[:5], axis=1)\n",
    "print(f\"User embedding norms (first 5): {user_norms}\")\n",
    "print(f\"Item embedding norms (first 5): {item_norms}\")\n",
    "print(f\"Should be ~1.0 if normalized\")\n",
    "\n",
    "# Check 2: What do similarity scores look like?\n",
    "print(\"\\n=== Sample Search ===\")\n",
    "test_user_idx = 0\n",
    "query = user_embeddings[test_user_idx:test_user_idx+1]\n",
    "distances, indices = index.search(query, k=10)\n",
    "print(f\"Top 10 similarities: {distances[0]}\")\n",
    "print(f\"Top 10 item indices: {indices[0]}\")\n",
    "print(f\"Similarities should be between 0 and 1, with variation\")\n",
    "\n",
    "# Check 3: Does user's actual item appear anywhere?\n",
    "print(\"\\n=== Ground Truth Check ===\")\n",
    "test_user_id = list(user2idx.keys())[0]\n",
    "test_user_items = test_df[test_df['user_id'] == test_user_id]['item_id'].values\n",
    "print(f\"User {test_user_id} test items: {test_user_items[:5]}\")\n",
    "if len(test_user_items) > 0 and test_user_items[0] in item2idx:\n",
    "    true_item_idx = item2idx[test_user_items[0]]\n",
    "    print(f\"True item index: {true_item_idx}\")\n",
    "    print(f\"Is true item in top 100? {true_item_idx in indices[0]}\")\n",
    "    \n",
    "    # Check similarity to true item\n",
    "    true_item_emb = item_embeddings[true_item_idx:true_item_idx+1]\n",
    "    similarity = np.dot(query, true_item_emb.T)[0][0]\n",
    "    print(f\"Similarity to true item: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA OVERLAP ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Test users: 322,870\n",
      "Test users found in training: 322,870\n",
      "Overlap: 100.0%\n",
      "\n",
      "Test items: 108,510\n",
      "Test items found in training: 81,079\n",
      "Overlap: 74.7%\n",
      "\n",
      "Total test interactions: 328,812\n",
      "Valid test interactions (user AND item in train): 298,228\n",
      "Valid percentage: 90.7%\n",
      "\n",
      "----------------------------------------\n",
      "FINDING VALID TEST CASES\n",
      "----------------------------------------\n",
      "\n",
      "User: 1 (idx: 609763)\n",
      "  True item: 154093 (idx: 14734)\n",
      "  Similarity to true item: 0.0317\n",
      "  Rank of true item: >1000 / 212,506\n",
      "\n",
      "User: 2 (idx: 399475)\n",
      "  True item: 135112 (idx: 77806)\n",
      "  Similarity to true item: -0.1700\n",
      "  Rank of true item: >1000 / 212,506\n",
      "\n",
      "User: 5 (idx: 74264)\n",
      "  True item: 45115 (idx: 28930)\n",
      "  Similarity to true item: -0.0566\n",
      "  Rank of true item: >1000 / 212,506\n",
      "\n",
      "Valid test cases in first 100 users: 86\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATA OVERLAP ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Check test users that exist in training\n",
    "test_users = test_df['user_id'].unique()\n",
    "train_users = set(user2idx.keys())\n",
    "test_users_in_train = [u for u in test_users if u in train_users]\n",
    "print(f\"\\nTest users: {len(test_users):,}\")\n",
    "print(f\"Test users found in training: {len(test_users_in_train):,}\")\n",
    "print(f\"Overlap: {len(test_users_in_train)/len(test_users)*100:.1f}%\")\n",
    "\n",
    "# 2. Check test items that exist in training\n",
    "test_items = test_df['item_id'].unique()\n",
    "train_items = set(item2idx.keys())\n",
    "test_items_in_train = [i for i in test_items if i in train_items]\n",
    "print(f\"\\nTest items: {len(test_items):,}\")\n",
    "print(f\"Test items found in training: {len(test_items_in_train):,}\")\n",
    "print(f\"Overlap: {len(test_items_in_train)/len(test_items)*100:.1f}%\")\n",
    "\n",
    "# 3. Check test interactions where BOTH user AND item are in training\n",
    "valid_test = test_df[\n",
    "    (test_df['user_id'].isin(train_users)) & \n",
    "    (test_df['item_id'].isin(train_items))\n",
    "]\n",
    "print(f\"\\nTotal test interactions: {len(test_df):,}\")\n",
    "print(f\"Valid test interactions (user AND item in train): {len(valid_test):,}\")\n",
    "print(f\"Valid percentage: {len(valid_test)/len(test_df)*100:.1f}%\")\n",
    "\n",
    "# 4. Find a valid test user for checking\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"FINDING VALID TEST CASES\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "valid_count = 0\n",
    "for user_id in test_users_in_train[:100]:  # Check first 100 valid users\n",
    "    user_items = test_df[test_df['user_id'] == user_id]['item_id'].values\n",
    "    valid_items = [i for i in user_items if i in train_items]\n",
    "    if valid_items:\n",
    "        valid_count += 1\n",
    "        if valid_count <= 3:  # Show first 3 examples\n",
    "            user_idx = user2idx[user_id]\n",
    "            item_idx = item2idx[valid_items[0]]\n",
    "            \n",
    "            # Check similarity\n",
    "            query = user_embeddings[user_idx:user_idx+1]\n",
    "            true_item_emb = item_embeddings[item_idx:item_idx+1]\n",
    "            similarity = np.dot(query, true_item_emb.T)[0][0]\n",
    "            \n",
    "            # Check rank\n",
    "            distances, indices = index.search(query, k=1000)\n",
    "            rank_pos = np.where(indices[0] == item_idx)[0]\n",
    "            rank = rank_pos[0] + 1 if len(rank_pos) > 0 else \">1000\"\n",
    "            \n",
    "            print(f\"\\nUser: {user_id} (idx: {user_idx})\")\n",
    "            print(f\"  True item: {valid_items[0]} (idx: {item_idx})\")\n",
    "            print(f\"  Similarity to true item: {similarity:.4f}\")\n",
    "            print(f\"  Rank of true item: {rank} / {index.ntotal:,}\")\n",
    "\n",
    "print(f\"\\nValid test cases in first 100 users: {valid_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with BPR Loss...\n",
      "Epoch 1/5 - Loss: 0.6875 - Time: 339.8s\n",
      "Epoch 2/5 - Loss: 0.5462 - Time: 339.5s\n",
      "Epoch 3/5 - Loss: 0.2789 - Time: 339.2s\n",
      "Epoch 4/5 - Loss: 0.1331 - Time: 339.3s\n",
      "Epoch 5/5 - Loss: 0.0682 - Time: 339.6s\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# IMPROVED TRAINING WITH BPR LOSS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Better initialization\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "    \n",
    "    def forward(self, user_idx, item_idx):\n",
    "        u = self.user_emb(user_idx)\n",
    "        i = self.item_emb(item_idx)\n",
    "        return (u * i).sum(dim=1)\n",
    "    \n",
    "    def get_embeddings(self, user_idx, item_idx):\n",
    "        u = self.user_emb(user_idx)\n",
    "        i = self.item_emb(item_idx)\n",
    "        return u, i\n",
    "\n",
    "# Reinitialize model\n",
    "embedding_dim = 64\n",
    "model = TwoTowerModel(num_users, num_items, embedding_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\"\"\"\n",
    "    BPR Loss: -log(sigmoid(pos_score - neg_score))\n",
    "    Directly optimizes: positive items should score higher than negatives\n",
    "\"\"\"\n",
    "def bpr_loss(pos_scores, neg_scores):\n",
    "    \n",
    "    return -F.logsigmoid(pos_scores - neg_scores).mean()\n",
    "\n",
    "print(\"Training with BPR Loss\")\n",
    "epochs = 5\n",
    "num_negatives = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for batch_users, batch_items in dataloader:\n",
    "        batch_users = batch_users.to(device)\n",
    "        batch_items = batch_items.to(device)\n",
    "        batch_size = batch_users.size(0)\n",
    "        \n",
    "        # Positive scores\n",
    "        pos_scores = model(batch_users, batch_items)\n",
    "        \n",
    "        # Multiple negatives and accumulate loss\n",
    "        loss = 0\n",
    "        for _ in range(num_negatives):\n",
    "            neg_items = torch.randint(0, num_items, (batch_size,), device=device)\n",
    "            neg_scores = model(batch_users, neg_items)\n",
    "            loss += bpr_loss(pos_scores, neg_scores)\n",
    "        \n",
    "        loss = loss / num_negatives  # Average over negatives\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Time: {elapsed:.1f}s\")\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings extracted and normalized\n",
      "  User embeddings: (1210271, 64)\n",
      "  Item embeddings: (212506, 64)\n",
      "FAISS index rebuilt: 212,506 items\n",
      "\n",
      "--- Quick Sanity Check ---\n",
      "User 1: Similarity=0.2778, Rank=>1000\n",
      "User 2: Similarity=-0.0600, Rank=>1000\n",
      "User 5: Similarity=0.0190, Rank=>1000\n",
      "\n",
      "============================================================\n",
      "FULL EVALUATION\n",
      "============================================================\n",
      "\n",
      "Evaluated 4,578 users\n",
      "\n",
      "Two-Tower Retrieval (BPR Loss):\n",
      "  HR@10: 0.0072 (0.72%)\n",
      "  HR@20: 0.0111 (1.11%)\n",
      "  HR@50: 0.0201 (2.01%)\n",
      "  HR@100: 0.0262 (2.62%)\n",
      "\n",
      "----------------------------------------\n",
      "COMPARISON\n",
      "----------------------------------------\n",
      "Most Popular: HR@10=0.0077, HR@20=0.0132, HR@50=0.0241\n",
      "Item-KNN:     HR@10=0.0123, HR@20=0.0180, HR@50=0.0294\n",
      "Two-Tower:    HR@10=0.0072, HR@20=0.0111, HR@50=0.0201\n",
      "\n",
      "❌ Still underperforming - may need more epochs or tuning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# STEP 1: Extract and Normalize Embeddings\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    user_emb_raw = model.user_emb.weight.detach().cpu().numpy().astype('float32')\n",
    "    item_emb_raw = model.item_emb.weight.detach().cpu().numpy().astype('float32')\n",
    "\n",
    "# Create normalized copies\n",
    "user_embeddings = user_emb_raw.copy()\n",
    "item_embeddings = item_emb_raw.copy()\n",
    "faiss.normalize_L2(user_embeddings)\n",
    "faiss.normalize_L2(item_embeddings)\n",
    "\n",
    "print(f\"Embeddings extracted and normalized\")\n",
    "print(f\"  User embeddings: {user_embeddings.shape}\")\n",
    "print(f\"  Item embeddings: {item_embeddings.shape}\")\n",
    "\n",
    "\n",
    "# STEP 2: Rebuild FAISS Index\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "cpu_index = faiss.IndexFlatIP(embedding_dim)\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "gpu_index.add(item_embeddings)\n",
    "index = gpu_index  # Update the index variable\n",
    "\n",
    "print(f\"FAISS index rebuilt: {index.ntotal:,} items\")\n",
    "\n",
    "\n",
    "# STEP 3: Full Evaluation (HR@K)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_hr_at_k(test_df, ks=(10, 20, 50, 100), max_users=5000):\n",
    "    \"\"\"Evaluate Hit Rate @ K.\"\"\"\n",
    "    hr = {k: 0 for k in ks}\n",
    "    total = 0\n",
    "    max_k = max(ks)\n",
    "    \n",
    "    test_users = test_df['user_id'].unique()\n",
    "    if len(test_users) > max_users:\n",
    "        np.random.seed(42)\n",
    "        test_users = np.random.choice(test_users, max_users, replace=False)\n",
    "    \n",
    "    for user_id in test_users:\n",
    "        if user_id not in user2idx:\n",
    "            continue\n",
    "        \n",
    "        user_idx = user2idx[user_id]\n",
    "        true_items = test_df[test_df['user_id'] == user_id]['item_id'].values\n",
    "        true_item_indices = [item2idx[item] for item in true_items if item in item2idx]\n",
    "        \n",
    "        if not true_item_indices:\n",
    "            continue\n",
    "        \n",
    "        query_emb = user_embeddings[user_idx:user_idx+1]\n",
    "        distances, indices = index.search(query_emb, k=max_k)\n",
    "        retrieved = indices[0]\n",
    "        \n",
    "        total += 1\n",
    "        \n",
    "        for k in ks:\n",
    "            if any(item_idx in retrieved[:k] for item_idx in true_item_indices):\n",
    "                hr[k] += 1\n",
    "    \n",
    "    for k in ks:\n",
    "        hr[k] = hr[k] / total if total > 0 else 0\n",
    "    \n",
    "    return hr, total\n",
    "\n",
    "hr, total_evaluated = evaluate_hr_at_k(test_df, ks=(10, 20, 50, 100), max_users=5000)\n",
    "\n",
    "print(f\"\\nEvaluated {total_evaluated:,} users\")\n",
    "print(\"\\nTwo-Tower Retrieval (BPR Loss):\")\n",
    "for k in sorted(hr.keys()):\n",
    "    print(f\"  HR@{k}: {hr[k]:.4f} ({hr[k]*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# STEP 5: Compare with Baselines\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"COMPARISON\")\n",
    "print(\"-\"*40)\n",
    "print(\"Most Popular: HR@10=0.0077, HR@20=0.0132, HR@50=0.0241\")\n",
    "print(\"Item-KNN:     HR@10=0.0123, HR@20=0.0180, HR@50=0.0294\")\n",
    "print(f\"Two-Tower:    HR@10={hr[10]:.4f}, HR@20={hr[20]:.4f}, HR@50={hr[50]:.4f}\")\n",
    "\n",
    "if hr[10] > 0.0123:\n",
    "    print(\"\\n SUCCESS! Two-Tower beats Item-KNN baseline!\")\n",
    "elif hr[10] > 0.0077:\n",
    "    print(\"\\n⚠ Two-Tower beats Most Popular but not Item-KNN\")\n",
    "else:\n",
    "    print(\"\\n Still underperforming - may need more epochs or tuning\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RecSys GPU",
   "language": "python",
   "name": "recsys_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
