{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFM Ranking (RecBole)\n",
    "\n",
    "\n",
    "- Loads configuration from `deepfm_config.yaml` and optional env-var overrides\n",
    "- Builds RecBole dataset/dataloaders, trains DeepFM, evaluates on the test split\n",
    "- Saves the best checkpoint + JSON metrics under a configurable directory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecBole imported successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.model.context_aware_recommender import DeepFM\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.utils import init_seed\n",
    "from recbole.utils.case_study import full_sort_topk\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    faiss = None\n",
    "    FAISS_AVAILABLE = False\n",
    "    print(\"FAISS not found. Falling back to numpy search for candidates.\")\n",
    "\n",
    "print(\"RecBole imported successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: amazon-beauty\n",
      "Checkpoint dir: saved_models/deepfm\n",
      "GPU ID: 0\n",
      "Use GPU: True\n",
      "Seed: 42\n"
     ]
    }
   ],
   "source": [
    "DATASET = os.environ.get(\"RECO_DATASET\", \"amazon-beauty\")\n",
    "CHECKPOINT_DIR = os.environ.get(\"CHECKPOINT_DIR\", \"saved_models/deepfm\")\n",
    "DATA_PATH = os.environ.get(\"DATA_PATH\") or os.environ.get(\"DATA_DIR\")\n",
    "GPU_ID = int(os.environ.get(\"GPU_ID\", 0))\n",
    "SEED = int(os.environ.get(\"SEED\", 42))\n",
    "USE_GPU = os.environ.get(\"USE_GPU\", \"true\").lower() != \"false\"\n",
    "\n",
    "config_overrides = {\n",
    "    \"checkpoint_dir\": CHECKPOINT_DIR,\n",
    "    \"gpu_id\": GPU_ID,\n",
    "    \"use_gpu\": USE_GPU,\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "if DATA_PATH:\n",
    "    config_overrides[\"data_path\"] = DATA_PATH\n",
    "\n",
    "print(\"Dataset:\", DATASET)\n",
    "print(\"Checkpoint dir:\", CHECKPOINT_DIR)\n",
    "print(\"GPU ID:\", GPU_ID)\n",
    "print(\"Use GPU:\", USE_GPU)\n",
    "print(\"Seed:\", SEED)\n",
    "if DATA_PATH:\n",
    "    print(\"Data path override:\", DATA_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config():\n",
    "    config = Config(\n",
    "        model=\"DeepFM\",\n",
    "        dataset=DATASET,\n",
    "        config_file_list=[\"deepfm_config.yaml\"],\n",
    "        config_dict=config_overrides,\n",
    "    )\n",
    "    print(\"Using device:\", config[\"device\"])\n",
    "    return config\n",
    "\n",
    "\n",
    "def run_deepfm_training():\n",
    "    config = build_config()\n",
    "    init_seed(config[\"seed\"], config[\"reproducibility\"])\n",
    "\n",
    "    print(\"Creating dataset\")\n",
    "    dataset = create_dataset(config)\n",
    "    print(f\"      Dataset created: {dataset}\")\n",
    "    \n",
    "    print(\"Preparing train/valid/test data\")\n",
    "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "    print(f\"      Train batches: {len(train_data)}, Valid batches: {len(valid_data)}, Test batches: {len(test_data)}\")\n",
    "\n",
    "    print(\"Creating DeepFM model\")\n",
    "    model = DeepFM(config, dataset).to(config[\"device\"])\n",
    "    print(f\"      Model created: {model.__class__.__name__}\")\n",
    "    \n",
    "    print(\"Initializing trainer\")\n",
    "    trainer = Trainer(config, model)\n",
    "    \n",
    "    print(\"Starting training\")\n",
    "    import sys; sys.stdout.flush()\n",
    "    best_valid_score, best_valid_result = trainer.fit(train_data, valid_data)\n",
    "    \n",
    "    # Load best checkpoint so we can reuse the model for re-ranking\n",
    "    best_ckpt = trainer.saved_model_file\n",
    "    if os.path.exists(best_ckpt):\n",
    "        state = torch.load(best_ckpt, map_location=config[\"device\"])\n",
    "        model.load_state_dict(state[\"state_dict\"])\n",
    "        if \"other_parameter\" in state:\n",
    "            model.load_other_parameter(state[\"other_parameter\"])\n",
    "        model.to(config[\"device\"])\n",
    "        model.eval()\n",
    "    else:\n",
    "        print(\"Warning: best checkpoint not found. Using current model state.\")\n",
    "\n",
    "    print(\"Evaluating on test set\")\n",
    "    try:\n",
    "        test_result = trainer.evaluate(test_data, load_best_model=True)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: No best model checkpoint found. Evaluating current model state.\")\n",
    "        test_result = trainer.evaluate(test_data, load_best_model=False)\n",
    "\n",
    "    results = {\n",
    "        \"best_valid_score\": best_valid_score,\n",
    "        \"best_valid_result\": best_valid_result,\n",
    "        \"test_result\": test_result,\n",
    "    }\n",
    "\n",
    "    os.makedirs(config[\"checkpoint_dir\"], exist_ok=True)\n",
    "    output_path = os.path.join(config[\"checkpoint_dir\"], \"deepfm_results.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(\"Best validation score:\", best_valid_score)\n",
    "    print(\"Best validation result:\", best_valid_result)\n",
    "    print(\"Test result:\", test_result)\n",
    "    print(\"Saved results to:\", output_path)\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"model\": model,\n",
    "        \"dataset\": dataset,\n",
    "        \"train_data\": train_data,\n",
    "        \"valid_data\": valid_data,\n",
    "        \"test_data\": test_data,\n",
    "        \"config\": config,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Creating dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guo.yunyu/.conda/envs/recsys_gpu/lib/python3.10/site-packages/recbole/data/dataset/dataset.py:501: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[field].fillna(value=\"\", inplace=True)\n",
      "/home/guo.yunyu/.conda/envs/recsys_gpu/lib/python3.10/site-packages/recbole/data/dataset/dataset.py:1217: FutureWarning: using <built-in function len> in Series.agg cannot aggregate and has been deprecated. Use Series.transform to keep behavior unchanged.\n",
      "  split_point = np.cumsum(feat[field].agg(len))[:-1]\n",
      "/home/guo.yunyu/.conda/envs/recsys_gpu/lib/python3.10/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  feat[field].fillna(value=0, inplace=True)\n",
      "/home/guo.yunyu/.conda/envs/recsys_gpu/lib/python3.10/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Dataset created: \u001b[1;35mamazon-beauty\u001b[0m\n",
      "\u001b[1;34mThe number of users\u001b[0m: 1210272\n",
      "\u001b[1;34mAverage actions of users\u001b[0m: 1.6715842980621696\n",
      "\u001b[1;34mThe number of items\u001b[0m: 259205\n",
      "\u001b[1;34mAverage actions of items\u001b[0m: 8.115848423822781\n",
      "\u001b[1;34mThe number of inters\u001b[0m: 2023070\n",
      "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99935511162327%\n",
      "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'timestamp', 'title', 'sales_type', 'sales_rank', 'categories', 'price', 'brand', 'label']\n",
      "Preparing train/valid/test data\n",
      "      Train batches: 753, Valid batches: 4084, Test batches: 8969\n",
      "Creating DeepFM model\n",
      "      Model created: DeepFM\n",
      "Initializing trainer\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guo.yunyu/.conda/envs/recsys_gpu/lib/python3.10/site-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n",
      "/tmp/ipykernel_3332452/1420480766.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(best_ckpt, map_location=config[\"device\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guo.yunyu/.conda/envs/recsys_gpu/lib/python3.10/site-packages/recbole/trainer/trainer.py:583: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation score: 0.9984\n",
      "Best validation result: OrderedDict([('recall@10', 0.9984), ('recall@20', 0.9996), ('ndcg@10', 0.8154), ('ndcg@20', 0.8158), ('hit@10', 1.0), ('hit@20', 1.0)])\n",
      "Test result: OrderedDict([('recall@10', 0.9991), ('recall@20', 0.9998), ('ndcg@10', 0.7915), ('ndcg@20', 0.7917), ('hit@10', 1.0), ('hit@20', 1.0)])\n",
      "Saved results to: saved_models/deepfm/deepfm_results.json\n",
      "CPU times: user 13min 32s, sys: 7.19 s, total: 13min 39s\n",
      "Wall time: 13min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "artifacts = run_deepfm_training()\n",
    "deepfm_results = artifacts[\"results\"]\n",
    "deepfm_model = artifacts[\"model\"]\n",
    "deepfm_dataset = artifacts[\"dataset\"]\n",
    "deepfm_test_data = artifacts[\"test_data\"]\n",
    "deepfm_config = artifacts[\"config\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recall@10\t0.9984(train)\t0.9991(test)\t99.8% of the time the true item appears in the top 10 candidates\n",
    "# ndcg@10\t0.8154(train)\t0.7915(test)\tMeasures ranking quality (weights top positions). Values around 0.8 are very high\n",
    "# hit@10\t1.0(train)\t1.0(test)\tThe true item is in the top 10 for every user (100%)\n",
    "# The DeepFM model is performing extremely well under the current evaluation (more than 99% recall with random negative sampling).\n",
    "# Metrics indicate the model consistently ranks the true item in the top 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Generation via Two-Tower + FAISS\n",
    "\n",
    "reuse the RecBole `.inter` splits to train a lightweight two-tower model, build a FAISS index over item embeddings, and stage top candidates per user. These candidates feed into DeepFM for re-ranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-tower universe -> users: 1,210,271, items: 249,274\n"
     ]
    }
   ],
   "source": [
    "INTER_DIR = Path(\"dataset/amazon-beauty\")\n",
    "TRAIN_INTER = INTER_DIR / \"amazon-beauty-train.inter\"\n",
    "VALID_INTER = INTER_DIR / \"amazon-beauty-valid.inter\"\n",
    "TEST_INTER = INTER_DIR / \"amazon-beauty-test.inter\"\n",
    "\n",
    "assert TRAIN_INTER.exists(), \"Missing RecBole split files. Run the dataset download first.\"\n",
    "\n",
    "use_cols = [\"user_id\", \"item_id\", \"label\", \"timestamp\"]\n",
    "train_df = pd.read_csv(TRAIN_INTER, sep=\"\\t\", usecols=use_cols)\n",
    "valid_df = pd.read_csv(VALID_INTER, sep=\"\\t\", usecols=use_cols)\n",
    "test_df = pd.read_csv(TEST_INTER, sep=\"\\t\", usecols=use_cols)\n",
    "\n",
    "# consistent string tokens (matches RecBole dataset tokens)\n",
    "for df in (train_df, valid_df, test_df):\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    df[\"item_id\"] = df[\"item_id\"].astype(str)\n",
    "\n",
    "def build_id_mappings(df_list):\n",
    "    unique_users = pd.concat([df[\"user_id\"] for df in df_list]).unique()\n",
    "    unique_items = pd.concat([df[\"item_id\"] for df in df_list]).unique()\n",
    "    user2idx = {u: idx for idx, u in enumerate(unique_users)}\n",
    "    item2idx = {i: idx for idx, i in enumerate(unique_items)}\n",
    "    idx2item = {idx: item for item, idx in item2idx.items()}\n",
    "    return user2idx, item2idx, idx2item\n",
    "\n",
    "user2idx, item2idx, idx2item = build_id_mappings([train_df, valid_df, test_df])\n",
    "num_users, num_items = len(user2idx), len(item2idx)\n",
    "print(f\"Two-tower universe -> users: {num_users:,}, items: {num_items:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerDataset(Dataset):\n",
    "    def __init__(self, df, user2idx, item2idx, num_items, neg_ratio=1, seed=42):\n",
    "        self.user = df[\"user_id\"].map(user2idx).values\n",
    "        self.item = df[\"item_id\"].map(item2idx).values\n",
    "        self.label = df[\"label\"].values\n",
    "        self.num_items = num_items\n",
    "        self.neg_ratio = neg_ratio\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user) * (1 + self.neg_ratio)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_idx = idx // (1 + self.neg_ratio)\n",
    "        is_pos = (idx % (1 + self.neg_ratio)) == 0\n",
    "        u = self.user[base_idx]\n",
    "        if is_pos:\n",
    "            i = self.item[base_idx]\n",
    "            y = 1.0\n",
    "        else:\n",
    "            i = self.rng.integers(0, self.num_items)\n",
    "            y = 0.0\n",
    "        return (\n",
    "            torch.tensor(u, dtype=torch.long),\n",
    "            torch.tensor(i, dtype=torch.long),\n",
    "            torch.tensor(y, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
    "        # Initialize embeddings\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        u = self.user_emb(users)\n",
    "        i = self.item_emb(items)\n",
    "        return (u * i).sum(dim=-1)\n",
    "\n",
    "\"\"\"\n",
    "    Train a two-tower retrieval model.\n",
    "    \n",
    "    \"\"\"\n",
    "def train_two_tower(train_df, user2idx, item2idx, num_items, epochs=10, batch_size=4096, lr=5e-4, neg_ratio=4, emb_dim=128):\n",
    "    \n",
    "    dataset = TwoTowerDataset(train_df, user2idx, item2idx, num_items, neg_ratio=neg_ratio)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training two-tower on {device}, {len(dataset):,} samples, emb_dim={emb_dim}, neg_ratio={neg_ratio}\")\n",
    "    \n",
    "    model = TwoTowerModel(len(user2idx), num_items, emb_dim=emb_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for users, items, labels in loader:\n",
    "            users = users.to(device)\n",
    "            items = items.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(users, items)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * len(users)\n",
    "        avg_loss = running_loss / len(dataset)\n",
    "        print(f\"Two-tower epoch {epoch+1}/{epochs} - loss {avg_loss:.4f}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(item_embeddings):\n",
    "    dim = item_embeddings.shape[1]\n",
    "    if FAISS_AVAILABLE:\n",
    "        faiss.normalize_L2(item_embeddings)\n",
    "        cpu_index = faiss.IndexFlatIP(dim)\n",
    "        gpu_enabled = False\n",
    "        if hasattr(faiss, \"get_num_gpus\") and faiss.get_num_gpus() > 0:\n",
    "            try:\n",
    "                res = faiss.StandardGpuResources()\n",
    "                gpu_id = int(os.environ.get(\"GPU_ID\", 0))\n",
    "                index = faiss.index_cpu_to_gpu(res, gpu_id, cpu_index)\n",
    "                gpu_enabled = True\n",
    "            except Exception as err:  # pragma: no cover\n",
    "                print(f\"Warning: falling back to CPU FAISS ({err})\")\n",
    "                index = cpu_index\n",
    "        else:\n",
    "            index = cpu_index\n",
    "        index.add(item_embeddings.astype(np.float32))\n",
    "        return index, gpu_enabled\n",
    "    else:\n",
    "        norms = np.linalg.norm(item_embeddings, axis=1, keepdims=True) + 1e-9\n",
    "        normalized = item_embeddings / norms\n",
    "        class NumpyIndex:\n",
    "            def __init__(self, emb):\n",
    "                self.emb = emb\n",
    "            def search(self, queries, k):\n",
    "                sims = queries @ self.emb.T\n",
    "                idx = np.argpartition(sims, -k, axis=1)[:, -k:]\n",
    "                part = np.take_along_axis(sims, idx, axis=1)\n",
    "                order = np.argsort(part, axis=1)[:, ::-1]\n",
    "                top_idx = np.take_along_axis(idx, order, axis=1)\n",
    "                top_scores = np.take_along_axis(sims, top_idx, axis=1)\n",
    "                return top_scores, top_idx\n",
    "        return NumpyIndex(normalized), False\n",
    "\n",
    "\n",
    "def generate_candidates(model, history_df, test_df, user2idx, item2idx, idx2item, candidate_topk=100, min_session_len=5, max_users=5000):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    user_emb = model.user_emb.weight.detach().cpu().numpy()\n",
    "    item_emb = model.item_emb.weight.detach().cpu().numpy()\n",
    "\n",
    "    # string tokens so stay consistent with RecBole and the .inter files\n",
    "    history_df = history_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    history_df[\"user_id\"] = history_df[\"user_id\"].astype(str)\n",
    "    test_df[\"user_id\"] = test_df[\"user_id\"].astype(str)\n",
    "    test_df[\"item_id\"] = test_df[\"item_id\"].astype(str)\n",
    "\n",
    "    index, used_faiss = build_faiss_index(item_emb.copy())\n",
    "    print(\"FAISS index\" if used_faiss else \"Numpy index\", \"will serve retrieval\")\n",
    "\n",
    "    hist_counts = history_df.groupby(\"user_id\").size()\n",
    "    eligible_users = hist_counts[hist_counts >= min_session_len].index\n",
    "\n",
    "    test_pos = test_df[test_df[\"label\"] == 1]\n",
    "    test_pos = test_pos[test_pos[\"user_id\"].isin(eligible_users)]\n",
    "    if max_users:\n",
    "        test_pos = test_pos.groupby(\"user_id\").head(1)\n",
    "        unique_users = test_pos[\"user_id\"].unique()\n",
    "        if len(unique_users) > max_users:\n",
    "            sampled = np.random.choice(unique_users, size=max_users, replace=False)\n",
    "            test_pos = test_pos[test_pos[\"user_id\"].isin(sampled)]\n",
    "\n",
    "    candidate_dict = {}\n",
    "    metrics_hits = {k: 0 for k in (5, 10, 20)}\n",
    "    total = 0\n",
    "\n",
    "    for row in test_pos.itertuples(index=False):\n",
    "        user_token = str(row.user_id)\n",
    "        item_token = str(row.item_id)\n",
    "        u_idx = user2idx.get(user_token)\n",
    "        i_idx = item2idx.get(item_token)\n",
    "        if u_idx is None or i_idx is None:\n",
    "            continue\n",
    "        user_vec = user_emb[u_idx]\n",
    "        user_vec = user_vec / (np.linalg.norm(user_vec) + 1e-9)\n",
    "        scores, idxs = index.search(user_vec.reshape(1, -1), candidate_topk)\n",
    "        top_items = [str(idx2item[idx]) for idx in idxs[0]]\n",
    "        candidate_dict[user_token] = top_items\n",
    "\n",
    "        total += 1\n",
    "        for k in metrics_hits.keys():\n",
    "            if item_token in top_items[:k]:\n",
    "                metrics_hits[k] += 1\n",
    "\n",
    "    if total == 0:\n",
    "        raise RuntimeError(\"No eligible users found for two-tower retrieval\")\n",
    "\n",
    "    retrieval_metrics = {f\"recall@{k}\": metrics_hits[k] / total for k in metrics_hits}\n",
    "    retrieval_metrics[\"num_users\"] = total\n",
    "    return candidate_dict, retrieval_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training two-tower on cuda, 7,706,605 samples, emb_dim=128, neg_ratio=4\n",
      "Two-tower epoch 1/10 - loss 0.6927\n",
      "Two-tower epoch 2/10 - loss 0.6585\n",
      "Two-tower epoch 3/10 - loss 0.5182\n",
      "Two-tower epoch 4/10 - loss 0.3772\n",
      "Two-tower epoch 5/10 - loss 0.2989\n",
      "Two-tower epoch 6/10 - loss 0.2577\n",
      "Two-tower epoch 7/10 - loss 0.2287\n",
      "Two-tower epoch 8/10 - loss 0.2029\n",
      "Two-tower epoch 9/10 - loss 0.1782\n",
      "Two-tower epoch 10/10 - loss 0.1544\n",
      "FAISS index will serve retrieval\n",
      "Retrieval metrics: {'recall@5': 0.0026, 'recall@10': 0.0034, 'recall@20': 0.0058, 'num_users': 5000}\n",
      "CPU times: user 17min 58s, sys: 11.2 s, total: 18min 9s\n",
      "Wall time: 25min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history_df = pd.concat([train_df, valid_df], ignore_index=True)\n",
    "\n",
    "\n",
    "two_tower_model = train_two_tower(\n",
    "    train_df, user2idx, item2idx, num_items,\n",
    "    epochs=10,       \n",
    "    batch_size=4096,\n",
    "    lr=5e-4,\n",
    "    neg_ratio=4,     \n",
    "    emb_dim=128      \n",
    ")\n",
    "\n",
    "candidate_dict, retrieval_metrics = generate_candidates(\n",
    "    two_tower_model,\n",
    "    history_df,\n",
    "    test_df,\n",
    "    user2idx,\n",
    "    item2idx,\n",
    "    idx2item,\n",
    "    candidate_topk=200,\n",
    "    min_session_len=5,\n",
    "    max_users=5000,\n",
    ")\n",
    "print(\"Retrieval metrics:\", retrieval_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 5000 test users, only 29 users (0.58%) had their ground-truth item in the top-200 candidates\n",
    "\n",
    "Reason: \n",
    "\t                \n",
    "249K items to search:\tFinding 1 needle in 249K haystack\n",
    "\n",
    "99.999% sparsity:\tEach user has only ~1.7 interactions\n",
    "\n",
    "Simple model:\tJust dot-product of embeddings\n",
    "\n",
    "Cold users:\tMany users have minimal history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-ranking 5000 users x ~200 candidates\n",
      "RecBole vocab: 1210272 users, 259205 items\n",
      "  User 0/5000...\n",
      "  User 1000/5000...\n",
      "  User 2000/5000...\n",
      "  User 3000/5000...\n",
      "  User 4000/5000...\n",
      "Done. Re-ranked 5000 users. Skipped 0.\n",
      "DeepFM re-ranked 5000 users\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\"\"\"\n",
    "    Re-rank two-tower candidates using DeepFM.\n",
    "    candidate_dict keys/values are RecBole internal indices \n",
    "    \"\"\"\n",
    "def rerank_candidates_with_deepfm(model, dataset, config, candidate_dict):\n",
    "    \n",
    "    from recbole.data.interaction import Interaction\n",
    "\n",
    "    device = config[\"device\"]\n",
    "    model.eval()\n",
    "\n",
    "    uid_field = dataset.uid_field\n",
    "    iid_field = dataset.iid_field\n",
    "    num_users = dataset.num(uid_field)\n",
    "    num_items = dataset.num(iid_field)\n",
    "\n",
    "    n_cands = len(next(iter(candidate_dict.values())))\n",
    "    print(f\"Re-ranking {len(candidate_dict)} users x ~{n_cands} candidates\")\n",
    "    print(f\"RecBole vocab: {num_users} users, {num_items} items\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    reranked = {}\n",
    "    skipped = 0\n",
    "    skip_reasons = {\"user_out_of_range\": 0, \"no_valid_items\": 0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (user_key, item_keys) in enumerate(candidate_dict.items()):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"  User {i}/{len(candidate_dict)}...\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # candidate_dict keys are RecBole internal indices (ints or str of ints)\n",
    "            try:\n",
    "                uid_internal = int(user_key)\n",
    "            except (ValueError, TypeError):\n",
    "                skipped += 1\n",
    "                skip_reasons[\"user_out_of_range\"] += 1\n",
    "                continue\n",
    "            if not (0 <= uid_internal < num_users):\n",
    "                skipped += 1\n",
    "                skip_reasons[\"user_out_of_range\"] += 1\n",
    "                continue\n",
    "\n",
    "            valid_pairs = []\n",
    "            for item_key in item_keys:\n",
    "                try:\n",
    "                    iid_internal = int(item_key)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                if 0 <= iid_internal < num_items:\n",
    "                    valid_pairs.append((iid_internal, item_key))\n",
    "            if not valid_pairs:\n",
    "                skipped += 1\n",
    "                skip_reasons[\"no_valid_items\"] += 1\n",
    "                continue\n",
    "\n",
    "            internal_iids, original_keys = zip(*valid_pairs)\n",
    "            n = len(internal_iids)\n",
    "            user_tensor = torch.full((n,), uid_internal, dtype=torch.long, device=device)\n",
    "            item_tensor = torch.tensor(internal_iids, dtype=torch.long, device=device)\n",
    "\n",
    "            interaction = Interaction({uid_field: user_tensor, iid_field: item_tensor})\n",
    "            interaction = dataset.join(interaction)\n",
    "            interaction = interaction.to(device)\n",
    "\n",
    "            scores = model.predict(interaction).cpu().numpy()\n",
    "            ranked_idx = np.argsort(-scores)\n",
    "            reranked[user_key] = [original_keys[j] for j in ranked_idx]\n",
    "\n",
    "    print(f\"Done. Re-ranked {len(reranked)} users. Skipped {skipped}.\")\n",
    "    if skipped:\n",
    "        print(\"Skip reasons:\", skip_reasons)\n",
    "    return reranked\n",
    "\n",
    "\n",
    "# Re-rank only two-tower candidates, keeps IDs consistent with RecBole tokens\n",
    "deepfm_reranked = rerank_candidates_with_deepfm(\n",
    "    deepfm_model, deepfm_dataset, deepfm_config, candidate_dict\n",
    ")\n",
    "print(f\"DeepFM re-ranked {len(deepfm_reranked)} users\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 5000 users\n",
      "Sample user keys: ['40', '62', '141']\n",
      "Test df user_id sample: ['1', '2', '5']\n",
      "\n",
      "=== Pipeline Results (TwoTower → DeepFM) ===\n",
      "  hit@5: 0.0025\n",
      "  hit@10: 0.0039\n",
      "  hit@20: 0.0071\n",
      "  recall@5: 0.0025\n",
      "  recall@10: 0.0039\n",
      "  recall@20: 0.0071\n",
      "  ndcg@5: 0.0016\n",
      "  ndcg@10: 0.0020\n",
      "  ndcg@20: 0.0028\n",
      "  num_users: 5883\n"
     ]
    }
   ],
   "source": [
    "# === Evaluation ===\n",
    "\n",
    "def evaluate_ranked_lists(ranked_dict, test_df, topks=(5, 10, 20)):\n",
    "    \"\"\"Evaluate ranked recommendations against test set ground truth.\"\"\"\n",
    "    pos_df = test_df[test_df[\"label\"] == 1]\n",
    "    pos_df = pos_df[pos_df[\"user_id\"].isin(ranked_dict.keys())]\n",
    "    metrics = {f\"hit@{k}\": 0 for k in topks}\n",
    "    metrics.update({f\"recall@{k}\": 0 for k in topks})\n",
    "    metrics.update({f\"ndcg@{k}\": 0.0 for k in topks})\n",
    "    total = 0\n",
    "    for row in pos_df.itertuples(index=False):\n",
    "        ranked = ranked_dict.get(row.user_id)\n",
    "        if not ranked:\n",
    "            continue\n",
    "        total += 1\n",
    "        for k in topks:\n",
    "            topk = ranked[:k]\n",
    "            if row.item_id in topk:\n",
    "                metrics[f\"hit@{k}\"] += 1\n",
    "                metrics[f\"recall@{k}\"] += 1\n",
    "                rank = topk.index(row.item_id)\n",
    "                metrics[f\"ndcg@{k}\"] += 1 / np.log2(rank + 2)\n",
    "    if total == 0:\n",
    "        raise RuntimeError(\"No overlapping users for ranked evaluation.\")\n",
    "    for k in topks:\n",
    "        metrics[f\"hit@{k}\"] /= total\n",
    "        metrics[f\"recall@{k}\"] /= total\n",
    "        metrics[f\"ndcg@{k}\"] /= total\n",
    "    metrics[\"num_users\"] = total\n",
    "    return metrics\n",
    "\n",
    "# top-20 for final evaluation (stored as string tokens)\n",
    "reranked_candidates = {str(u): [str(i) for i in items[:20]] for u, items in deepfm_reranked.items()}\n",
    "\n",
    "print(f\"Evaluating {len(reranked_candidates)} users\")\n",
    "print(f\"Sample user keys: {list(reranked_candidates.keys())[:3]}\")\n",
    "\n",
    "# test_df types\n",
    "test_eval_df = test_df.copy()\n",
    "test_eval_df[\"user_id\"] = test_eval_df[\"user_id\"].astype(str)\n",
    "test_eval_df[\"item_id\"] = test_eval_df[\"item_id\"].astype(str)\n",
    "print(f\"Test df user_id sample: {test_eval_df['user_id'].head(3).tolist()}\")\n",
    "\n",
    "pipeline_metrics = evaluate_ranked_lists(reranked_candidates, test_eval_df, topks=(5, 10, 20))\n",
    "print(\"\\n=== Pipeline Results (TwoTower → DeepFM) ===\")\n",
    "for k, v in pipeline_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoTower: recall@20 = 0.0058\n",
    "\n",
    "TwoTower+DeepFM: recall@20 = 0.0071 (+22% improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>DeepFM_full</th>\n",
       "      <th>TwoTower</th>\n",
       "      <th>TwoTower+DeepFM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>recall@5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.002550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ndcg@5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall@10</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.003910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ndcg@10</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recall@20</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.007139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ndcg@20</td>\n",
       "      <td>0.7917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>num_users_eval</td>\n",
       "      <td>322870.0000</td>\n",
       "      <td>5000.0000</td>\n",
       "      <td>5883.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric  DeepFM_full   TwoTower  TwoTower+DeepFM\n",
       "0        recall@5          NaN     0.0026         0.002550\n",
       "1          ndcg@5          NaN        NaN         0.001574\n",
       "2       recall@10       0.9991     0.0034         0.003910\n",
       "3         ndcg@10       0.7915        NaN         0.002002\n",
       "4       recall@20       0.9998     0.0058         0.007139\n",
       "5         ndcg@20       0.7917        NaN         0.002822\n",
       "6  num_users_eval  322870.0000  5000.0000      5883.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_metric(result_dict, metric_name):\n",
    "    for key, value in result_dict.items():\n",
    "        if key.lower() == metric_name.lower():\n",
    "            return float(value)\n",
    "    return np.nan\n",
    "\n",
    "comparison_rows = []\n",
    "for k in (5, 10, 20):\n",
    "    comparison_rows.append({\n",
    "        \"metric\": f\"recall@{k}\",\n",
    "        \"DeepFM_full\": extract_metric(deepfm_results[\"test_result\"], f\"Recall@{k}\"),\n",
    "        \"TwoTower\": retrieval_metrics.get(f\"recall@{k}\", np.nan),\n",
    "        \"TwoTower+DeepFM\": pipeline_metrics.get(f\"recall@{k}\", np.nan),\n",
    "    })\n",
    "    comparison_rows.append({\n",
    "        \"metric\": f\"ndcg@{k}\",\n",
    "        \"DeepFM_full\": extract_metric(deepfm_results[\"test_result\"], f\"NDCG@{k}\"),\n",
    "        \"TwoTower\": np.nan,  # retrieval computed only recall@K\n",
    "        \"TwoTower+DeepFM\": pipeline_metrics.get(f\"ndcg@{k}\", np.nan),\n",
    "    })\n",
    "comparison_rows.append({\n",
    "    \"metric\": \"num_users_eval\",\n",
    "    \"DeepFM_full\": len(test_df[test_df[\"label\"] == 1][\"user_id\"].unique()),\n",
    "    \"TwoTower\": retrieval_metrics[\"num_users\"],\n",
    "    \"TwoTower+DeepFM\": pipeline_metrics[\"num_users\"],\n",
    "})\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "display(comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althought DeepFM has significantly hight recall@10, but this is not a fair comparison\n",
    "\n",
    "DeepFM_full: Evaluated against 5 random negatives (easy, due to time constraint)\n",
    "\n",
    "TwoTower + DeepFM: Evaluated against 249K items (hard)\n",
    "\n",
    "The pipeline is technically correct, but the two-tower retrieval is too weak for this extremely sparse dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSys GPU",
   "language": "python",
   "name": "recsys_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
